# Setup
Below is the setup on a MacOS machine running Python 3.11. First, run 
```cd server``` followed by ```python3 -m venv venv``` and ```source venv/bin/activate``` to setup and start a virtual environment. Next, ensure ```ollama``` is installed locally, and run ```$ ollama pull gemma2:2b``` in your terminal outside of the venv. Next, install all dependencies using
```pip install -r requirements.txt```. Once this is complete, run the server with ```flask run```. Next, open up a new terminal and run ```cd client``` followed by ```npm install``` and ```npm run dev``` to run the UI locally.

# Response
My general approach was to set up a vector database with each document as an entry to enable retrieval. This was done using ```ChromaDB``` and the ```all-MiniLM-L6-v2``` transformer from HuggingFace. This setup is done when the server is started. Then, once the server recieves a message from the user via the UI, the top 3 most similar documents were retrieved from the database and formatted into a prompt. I created a langchain agent with a locally ran Ollama model as the backbone. My main focus from here was implementing the ```create_ticket``` tool as Ollama models do no support langchain tools directly. To work around this, I updated the system prompt to tell the model to add a formatted string to the top of any reponse that required creating a ticket. If such a string was present in the response, a ticket is manually created using the data provided by the model, and the resulting ticket information is added to the response.

I decided to use Gemma 2:2b as I wanted to use a local model, and because it performs better at general knowledge when compared to similar sized models like Llama 3.2. However, with more time, I would've setup a larger model to use. Gemma2 was large enough to generally provide good answers, but sometimes had a difficult time following the system propmpt despite clear instructions. A larger model would have provided better performance and efficiency. Another drawback was the lack of langchain tool capabilities with Ollama models, and using a different model (that supports such tool) would've eliminated this issue. Another small thing that I would've changed with more time is the UI. Adding a small textbox that shows when the model is responding after being prompted would make the user experience better.